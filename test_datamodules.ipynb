{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.plugins.training_type import DeepSpeedPlugin, DDPPlugin\n",
    "from pytorch_lightning.plugins.environments import SLURMEnvironment\n",
    "import torch\n",
    "from flatten_dict import flatten, unflatten\n",
    "import yaml\n",
    "import ml_collections as mlc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfold.config import model_config\n",
    "from openfold.data.data_modules import (\n",
    "    OpenFoldDataModule,\n",
    "    DummyDataLoader,\n",
    ")\n",
    "from openfold.model.model import AlphaFold\n",
    "from openfold.model.torchscript import script_preset_\n",
    "from openfold.np import residue_constants\n",
    "from openfold.utils.argparse import remove_arguments\n",
    "from openfold.utils.callbacks import (\n",
    "    EarlyStoppingVerbose,\n",
    ")\n",
    "from openfold.utils.exponential_moving_average import ExponentialMovingAverage\n",
    "from openfold.utils.loss import AlphaFoldLoss, lddt_ca\n",
    "from openfold.utils.lr_schedulers import AlphaFoldLRScheduler\n",
    "from openfold.utils.seed import seed_everything\n",
    "from openfold.utils.superimposition import superimpose\n",
    "from openfold.utils.tensor_utils import tensor_tree_map\n",
    "from openfold.utils.validation_metrics import (\n",
    "    drmsd,\n",
    "    gdt_ts,\n",
    "    gdt_ha,\n",
    ")\n",
    "from openfold.utils.import_weights import import_jax_weights_\n",
    "\n",
    "from scripts.zero_to_fp32 import (\n",
    "    get_fp32_state_dict_from_zero_checkpoint,\n",
    "    get_global_step_from_zero_checkpoint,\n",
    ")\n",
    "\n",
    "from openfold.utils.logger import PerformanceLoggingCallback\n",
    "from openfold.utils.config_check import enforce_config_constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_openfold import OpenFoldWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  --wandb \\\n",
    "    #  --wandb_entity openfold \\\n",
    "    #  --wandb_project single_sequence_yiming  \\\n",
    "    #  --experiment_name no_msa_no_template \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    # train\n",
    "    train_data_dir=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/pdb_mmcif/mmcif_files\",\n",
    "    train_alignment_dir=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/alignment_db\",\n",
    "    alignment_index_path=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/gustaf/chain_lists/duplicated_super_fix.index\",\n",
    "    obsolete_pdbs_file_path=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/pdb_mmcif/obsolete.dat\",\n",
    "    use_small_bfd=False,\n",
    "    train_filter_path=None,\n",
    "    train_chain_data_cache_path=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/gustaf/prot_data_cache.json\",\n",
    "    # template\n",
    "    template_mmcif_dir=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/pdb_mmcif/mmcif_files\",\n",
    "    max_template_date=\"2021-10-01\",\n",
    "    template_release_dates_cache_path=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/gustaf/mmcif_cache.json\",\n",
    "    kalign_binary_path=\"/usr/bin/kalign\",\n",
    "    # validation\n",
    "    val_data_dir=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/gustaf/val_set/data\",\n",
    "    val_alignment_dir=\"/scratch/00946/zzhang/data/openfold/ls6-tacc/gustaf/val_set/alignments\",\n",
    "    # model\n",
    "    # {initial_training/fintuning/fintuning_no_template/model_1.1/model_1.2/model_1.1.1/model_1.1.2/model_1.2.1/model_1.2.2/model_1.2.3}\n",
    "    config_stage=\"initial_training\",\n",
    "    # {ptm/None}\n",
    "    config_ptm=False,\n",
    "    # {train/inference_long_seq/None}\n",
    "    config_mode=\"train\",\n",
    "    # {low_prec/None}\n",
    "    config_lowprec=False,\n",
    "    script_modules=False,\n",
    "    # ditillation\n",
    "    distillation_data_dir=None,\n",
    "    distillation_alignment_dir=None,\n",
    "    distillation_filter_path=None,\n",
    "    distillation_alignment_index_path=None,\n",
    "    _distillation_structure_index_path=None,\n",
    "    distillation_chain_data_cache_path=None,\n",
    "    # logging\n",
    "    log_lr=True,\n",
    "    checkpoint_every_epoch=True,\n",
    "    output_dir=\"train_baseline/test\",\n",
    "    log_performance=False,\n",
    "    wandb=True,\n",
    "    wandb_entity=\"openfold\",\n",
    "    wandb_project=\"single_sequence_yiming\",\n",
    "    experiment_name=\"no_msa_no_template\",\n",
    "    wandb_id=None,\n",
    "    # parallel\n",
    "    gpus=3,\n",
    "    num_nodes=1,\n",
    "    replace_sampler_ddp=True,\n",
    "    deepspeed_config_path=\"deepspeed_config.json\",\n",
    "    # trainer\n",
    "    seed=42,\n",
    "    train_epoch_len=126000,\n",
    "    accumulate_grad_batches=3,\n",
    "    num_sanity_val_steps=0,\n",
    "    reload_dataloaders_every_n_epochs = 1,\n",
    "    resume_from_ckpt=None,\n",
    "    resume_model_weights_only=False,\n",
    "    # early stopping\n",
    "    early_stopping=False,\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"configs/baseline.yaml\", \"w\") as f:\n",
    "#     yaml.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_arg_constrains(args):\n",
    "    if args[\"seed\"] is None and (\n",
    "        (args[\"gpus\"] is not None and args[\"gpus\"] > 1)\n",
    "        or (args[\"num_nodes\"] is not None and args[\"num_nodes\"] > 1)\n",
    "    ):\n",
    "        raise ValueError(\"For distributed training, --seed must be specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforce_arg_constrains(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "if args[\"seed\"] is not None:\n",
    "    seed_everything(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "for preset in [\"base\"] + args[\"config_preset\"].split(\"-\"):\n",
    "    if not preset:\n",
    "        continue\n",
    "    with open(f\"configs/{preset}.json\") as f:\n",
    "        config = unflatten({**flatten(config), **flatten(json.load(f))})\n",
    "# enforce_config_constraints(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"data_module\"][\"data_loaders\"] = {\n",
    "    \"batch_size\": 1,\n",
    "    \"num_workers\": 8,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "config[\"globals\"][\"chunk_size\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training mode (train_data_dir passed)\n",
    "data_module = OpenFoldDataModule(\n",
    "    config=mlc.ConfigDict(config[\"data\"]),\n",
    "    template_mmcif_dir=args[\"template_mmcif_dir\"],\n",
    "    max_template_date=args[\"max_template_date\"],\n",
    "    train_data_dir=args[\"train_data_dir\"],\n",
    "    train_alignment_dir=args[\"train_alignment_dir\"],\n",
    "    train_chain_data_cache_path=args[\"train_chain_data_cache_path\"],\n",
    "    distillation_data_dir=args[\"distillation_data_dir\"],\n",
    "    distillation_alignment_dir=args[\"distillation_alignment_dir\"],\n",
    "    distillation_chain_data_cache_path=args[\"distillation_chain_data_cache_path\"],\n",
    "    val_data_dir=args[\"val_data_dir\"],\n",
    "    val_alignment_dir=args[\"val_alignment_dir\"],\n",
    "    kalign_binary_path=args[\"kalign_binary_path\"],\n",
    "    train_filter_path=args[\"train_filter_path\"],\n",
    "    distillation_filter_path=args[\"distillation_filter_path\"],\n",
    "    obsolete_pdbs_file_path=args[\"obsolete_pdbs_file_path\"],\n",
    "    template_release_dates_cache_path=args[\"template_release_dates_cache_path\"],\n",
    "    batch_seed=args[\"seed\"],\n",
    "    train_epoch_len=args[\"train_epoch_len\"],\n",
    "    _distillation_structure_index_path=args[\"_distillation_structure_index_path\"],\n",
    "    alignment_index_path=args[\"alignment_index_path\"],\n",
    "    distillation_alignment_index_path=args[\"distillation_alignment_index_path\"],\n",
    "    predict_data_dir=None,\n",
    "    predict_alignment_dir=None,\n",
    "    # **vars(args)\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aatype \t\t\t torch.Size([1, 209, 4])\n",
      "residue_index \t\t\t torch.Size([1, 209, 4])\n",
      "seq_length \t\t\t torch.Size([1, 4])\n",
      "all_atom_positions \t\t\t torch.Size([1, 209, 37, 3, 4])\n",
      "all_atom_mask \t\t\t torch.Size([1, 209, 37, 4])\n",
      "resolution \t\t\t torch.Size([1, 4])\n",
      "is_distillation \t\t\t torch.Size([1, 4])\n",
      "template_aatype \t\t\t torch.Size([1, 4, 209, 4])\n",
      "template_all_atom_mask \t\t\t torch.Size([1, 4, 209, 37, 4])\n",
      "template_all_atom_positions \t\t\t torch.Size([1, 4, 209, 37, 3, 4])\n",
      "template_sum_probs \t\t\t torch.Size([1, 4, 1, 4])\n",
      "seq_mask \t\t\t torch.Size([1, 209, 4])\n",
      "msa_mask \t\t\t torch.Size([1, 128, 209, 4])\n",
      "msa_row_mask \t\t\t torch.Size([1, 128, 4])\n",
      "template_mask \t\t\t torch.Size([1, 4, 4])\n",
      "template_pseudo_beta \t\t\t torch.Size([1, 4, 209, 3, 4])\n",
      "template_pseudo_beta_mask \t\t\t torch.Size([1, 4, 209, 4])\n",
      "template_torsion_angles_sin_cos \t\t\t torch.Size([1, 4, 209, 7, 2, 4])\n",
      "template_alt_torsion_angles_sin_cos \t\t\t torch.Size([1, 4, 209, 7, 2, 4])\n",
      "template_torsion_angles_mask \t\t\t torch.Size([1, 4, 209, 7, 4])\n",
      "atom14_atom_exists \t\t\t torch.Size([1, 209, 14, 4])\n",
      "residx_atom14_to_atom37 \t\t\t torch.Size([1, 209, 14, 4])\n",
      "residx_atom37_to_atom14 \t\t\t torch.Size([1, 209, 37, 4])\n",
      "atom37_atom_exists \t\t\t torch.Size([1, 209, 37, 4])\n",
      "atom14_gt_exists \t\t\t torch.Size([1, 209, 14, 4])\n",
      "atom14_gt_positions \t\t\t torch.Size([1, 209, 14, 3, 4])\n",
      "atom14_alt_gt_positions \t\t\t torch.Size([1, 209, 14, 3, 4])\n",
      "atom14_alt_gt_exists \t\t\t torch.Size([1, 209, 14, 4])\n",
      "atom14_atom_is_ambiguous \t\t\t torch.Size([1, 209, 14, 4])\n",
      "rigidgroups_gt_frames \t\t\t torch.Size([1, 209, 8, 4, 4, 4])\n",
      "rigidgroups_gt_exists \t\t\t torch.Size([1, 209, 8, 4])\n",
      "rigidgroups_group_exists \t\t\t torch.Size([1, 209, 8, 4])\n",
      "rigidgroups_group_is_ambiguous \t\t\t torch.Size([1, 209, 8, 4])\n",
      "rigidgroups_alt_gt_frames \t\t\t torch.Size([1, 209, 8, 4, 4, 4])\n",
      "pseudo_beta \t\t\t torch.Size([1, 209, 3, 4])\n",
      "pseudo_beta_mask \t\t\t torch.Size([1, 209, 4])\n",
      "backbone_rigid_tensor \t\t\t torch.Size([1, 209, 4, 4, 4])\n",
      "backbone_rigid_mask \t\t\t torch.Size([1, 209, 4])\n",
      "chi_angles_sin_cos \t\t\t torch.Size([1, 209, 4, 2, 4])\n",
      "chi_mask \t\t\t torch.Size([1, 209, 4, 4])\n",
      "extra_msa \t\t\t torch.Size([1, 1024, 209, 4])\n",
      "extra_msa_mask \t\t\t torch.Size([1, 1024, 209, 4])\n",
      "extra_msa_row_mask \t\t\t torch.Size([1, 1024, 4])\n",
      "bert_mask \t\t\t torch.Size([1, 128, 209, 4])\n",
      "true_msa \t\t\t torch.Size([1, 128, 209, 4])\n",
      "extra_has_deletion \t\t\t torch.Size([1, 1024, 209, 4])\n",
      "extra_deletion_value \t\t\t torch.Size([1, 1024, 209, 4])\n",
      "msa_feat \t\t\t torch.Size([1, 128, 209, 49, 4])\n",
      "target_feat \t\t\t torch.Size([1, 209, 22, 4])\n",
      "batch_idx \t\t\t torch.Size([1, 4])\n",
      "use_clamped_fape \t\t\t torch.Size([1, 4])\n",
      "no_recycling_iters \t\t\t torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in sample.items():\n",
    "    print(k, \"\\t\\t\\t\", v.shape if hasattr(v, \"shape\") else type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isdir(args[\"resume_from_ckpt\"]):\n",
    "#         sd = get_fp32_state_dict_from_zero_checkpoint(args[\"resume_from_ckpt\"])\n",
    "#     else:\n",
    "#         sd = torch.load(args[\"resume_from_ckpt\"])\n",
    "#     sd = {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
    "#     model_module.load_state_dict(sd)\n",
    "#     logging.info(\"Successfully loaded model weights...\")\n",
    "\n",
    "# # TorchScript components of the model\n",
    "# if args[\"script_modules\"]:\n",
    "#     script_preset_(model_module)\n",
    "\n",
    "def get_model_basename(model_path):\n",
    "    return os.path.splitext(os.path.basename(os.path.normpath(model_path)))[0]\n",
    "\n",
    "path = \"af2/params/params_model_2_ptm.npz\"\n",
    "model_basename = get_model_basename(path)\n",
    "model_version = \"_\".join(model_basename.split(\"_\")[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module = OpenFoldWrapper(mlc.ConfigDict(config)).eval()\n",
    "\n",
    "# from openfold_original.model.model import AlphaFold\n",
    "\n",
    "# model_module.model = AlphaFold()\n",
    "\n",
    "import_jax_weights_(\n",
    "    model_module.model, path, version=model_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"template_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 209, 209, 64]) torch.Size([1, 209, 209, 128])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StructureModule' object has no attribute 'linear_q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_631876/210301527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_tree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openfold/lib/conda/envs/openfold_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/09101/whatever/openfold/openfold/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0;31m#######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                     \u001b[0;31m# [*, N_res, H * C_hidden]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m                     \u001b[0mkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openfold/lib/conda/envs/openfold_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StructureModule' object has no attribute 'linear_q'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch = sample\n",
    "    # At the start of validation, load the EMA weights\n",
    "    if model_module.cached_weights is None:\n",
    "        # model.state_dict() contains references to model weights rather\n",
    "        # than copies. Therefore, we need to clone them before calling\n",
    "        # load_state_dict().\n",
    "        clone_param = lambda t: t.detach().clone()\n",
    "        model_module.cached_weights = tensor_tree_map(clone_param, model_module.model.state_dict())\n",
    "        model_module.model.load_state_dict(model_module.ema.state_dict()[\"params\"])\n",
    "\n",
    "    # Run the model\n",
    "    outputs = model_module.model(batch)\n",
    "    batch = tensor_tree_map(lambda t: t[..., -1], batch)\n",
    "\n",
    "    # Compute loss and other metrics\n",
    "    batch[\"use_clamped_fape\"] = 0.0\n",
    "    _, loss_breakdown = model_module.loss(outputs, batch, _return_breakdown=True)\n",
    "    other_metrics = model_module._compute_validation_metrics(\n",
    "        batch, outputs, superimposition_metrics=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_metrics, loss_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    config_preset = args[config_preset]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "model_module = OpenFoldWrapper(config)\n",
    "if args.resume_from_ckpt:\n",
    "    if os.path.isdir(args.resume_from_ckpt):\n",
    "        last_global_step = get_global_step_from_zero_checkpoint(\n",
    "            args.resume_from_ckpt\n",
    "        )\n",
    "    else:\n",
    "        sd = torch.load(args.resume_from_ckpt)\n",
    "        last_global_step = int(sd[\"global_step\"])\n",
    "    model_module.resume_last_lr_step(last_global_step)\n",
    "    logging.info(\"Successfully loaded last lr step...\")\n",
    "if args.resume_from_ckpt and args.resume_model_weights_only:\n",
    "    if os.path.isdir(args.resume_from_ckpt):\n",
    "        sd = get_fp32_state_dict_from_zero_checkpoint(args.resume_from_ckpt)\n",
    "    else:\n",
    "        sd = torch.load(args.resume_from_ckpt)\n",
    "    sd = {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
    "    model_module.load_state_dict(sd)\n",
    "    logging.info(\"Successfully loaded model weights...\")\n",
    "\n",
    "# TorchScript components of the model\n",
    "if args.script_modules:\n",
    "    script_preset_(model_module)\n",
    "\n",
    "# data_module = DummyDataLoader(\"new_batch.pickle\")\n",
    "data_module = OpenFoldDataModule(\n",
    "    config=config.data, batch_seed=args.seed, **vars(args)\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "callbacks = []\n",
    "if args.checkpoint_every_epoch:\n",
    "    mc = ModelCheckpoint(\n",
    "        every_n_epochs=1,\n",
    "        auto_insert_metric_name=False,\n",
    "        save_top_k=-1,\n",
    "    )\n",
    "    callbacks.append(mc)\n",
    "\n",
    "if args.early_stopping:\n",
    "    es = EarlyStoppingVerbose(\n",
    "        monitor=\"val/lddt_ca\",\n",
    "        min_delta=args.min_delta,\n",
    "        patience=args.patience,\n",
    "        verbose=False,\n",
    "        mode=\"max\",\n",
    "        check_finite=True,\n",
    "        strict=True,\n",
    "    )\n",
    "    callbacks.append(es)\n",
    "\n",
    "if args.log_performance:\n",
    "    global_batch_size = args.num_nodes * args.gpus\n",
    "    perf = PerformanceLoggingCallback(\n",
    "        log_file=os.path.join(args.output_dir, \"performance_log.json\"),\n",
    "        global_batch_size=global_batch_size,\n",
    "    )\n",
    "    callbacks.append(perf)\n",
    "\n",
    "if args.log_lr:\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    callbacks.append(lr_monitor)\n",
    "\n",
    "loggers = []\n",
    "if args.wandb:\n",
    "    wdb_logger = WandbLogger(\n",
    "        name=args.experiment_name,\n",
    "        save_dir=args.output_dir,\n",
    "        id=args.wandb_id,\n",
    "        project=args.wandb_project,\n",
    "        **{\"entity\": args.wandb_entity},\n",
    "    )\n",
    "    loggers.append(wdb_logger)\n",
    "\n",
    "if args.deepspeed_config_path is not None:\n",
    "    strategy = DeepSpeedPlugin(\n",
    "        config=args.deepspeed_config_path,\n",
    "    )\n",
    "    if args.wandb:\n",
    "        wdb_logger.experiment.save(args.deepspeed_config_path)\n",
    "        wdb_logger.experiment.save(\"openfold/config.py\")\n",
    "elif (args.gpus is not None and args.gpus > 1) or args.num_nodes > 1:\n",
    "    strategy = DDPPlugin(find_unused_parameters=False)\n",
    "else:\n",
    "    strategy = None\n",
    "\n",
    "if args.wandb:\n",
    "    freeze_path = f\"{wdb_logger.experiment.dir}/package_versions.txt\"\n",
    "    os.system(f\"{sys.executable} -m pip freeze > {freeze_path}\")\n",
    "    wdb_logger.experiment.save(f\"{freeze_path}\")\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(\n",
    "    args,\n",
    "    default_root_dir=args.output_dir,\n",
    "    strategy=strategy,\n",
    "    callbacks=callbacks,\n",
    "    logger=loggers,\n",
    ")\n",
    "\n",
    "if args.resume_model_weights_only:\n",
    "    ckpt_path = None\n",
    "else:\n",
    "    ckpt_path = args.resume_from_ckpt\n",
    "\n",
    "trainer.fit(\n",
    "    model_module,\n",
    "    datamodule=data_module,\n",
    "    ckpt_path=ckpt_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe3a2011d36ca24623b5191badad917ca71244ac57d4f631470bf86b6b0818d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
