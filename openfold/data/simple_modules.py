import copy
from functools import partial
import json
import logging
import os
from typing import Mapping, Optional, Sequence, List, Any, Dict

import numpy as np
from scipy.spatial.transform import Rotation as R
import torch
import pytorch_lightning as pl
import torch
from torch.utils.data import RandomSampler
import ml_collections as mlc

from openfold.data import (
    templates,
    parsers,
    mmcif_parsing,
    feature_pipeline,
    data_pipeline,
)
from openfold.np import residue_constants as rc, protein
from openfold.data.data_pipeline import (
    _aatype_to_str_sequence,
    make_sequence_features,
    empty_template_feats,
)
from openfold.data.feature_pipeline import make_data_config, np_to_tensor_dict
from openfold.data.input_pipeline import nonensembled_transform_fns, compose
from openfold.utils.rigid_utils import Rotation, Rigid
from openfold.utils.feats import (
    frames_and_literature_positions_to_atom14_pos,
    torsion_angles_to_frames,
)
from openfold.np.residue_constants import (
    restype_rigid_group_default_frame as default_frames,
    restype_atom14_to_rigid_group as group_idx,
    restype_atom14_mask as atom_mask,
    restype_atom14_rigid_group_positions as lit_positions,
    restype_atom37_mask as atom_mask_37,
)
from openfold.utils.tensor_utils import batched_gather

FeatureDict = Mapping[str, np.ndarray]


class SimpleDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        data_dir: str,
        alignment_dir: str,
        template_mmcif_dir: str,
        max_template_date: str,
        config: mlc.ConfigDict,
        chain_data_cache_path: Optional[str] = None,
        kalign_binary_path: str = "/usr/bin/kalign",
        max_template_hits: int = 4,
        obsolete_pdbs_file_path: Optional[str] = None,
        template_release_dates_cache_path: Optional[str] = None,
        shuffle_top_k_prefiltered: Optional[int] = None,
        treat_pdb_as_distillation: bool = True,
        filter_path: Optional[str] = None,
        mode: str = "train",
        alignment_index: Optional[Any] = None,
        _output_raw: bool = False,
        _structure_index: Optional[Any] = None,
    ):
        """
        The most basic dataset. It is responsible for parsing structure, MSA and template data.
        It faithfully loops over the directory and parse everything with specified postfix.
        It takes care of MSA block deletion and cropping, which is sample-level.

        Args:
            data_dir:
                A path to a directory containing mmCIF files (in train
                mode) or FASTA files (in inference mode).
            alignment_dir:
                A path to a directory containing only data in the format
                output by an AlignmentRunner
                (defined in openfold.features.alignment_runner).
                I.e. a directory of directories named {PDB_ID}_{CHAIN_ID}
                or simply {PDB_ID}, each containing .a3m, .sto, and .hhr
                files.
            template_mmcif_dir:
                Path to a directory containing template mmCIF files.
            config:
                A dataset config object. See openfold.config
            chain_data_cache_path:
                Path to cache of data_dir generated by
                scripts/generate_chain_data_cache.py
            kalign_binary_path:
                Path to kalign binary.
            max_template_hits:
                An upper bound on how many templates are considered. During
                training, the templates ultimately used are subsampled
                from this total quantity.
            template_release_dates_cache_path:
                Path to the output of scripts/generate_mmcif_cache.
            obsolete_pdbs_file_path:
                Path to the file containing replacements for obsolete PDBs.
            shuffle_top_k_prefiltered:
                Whether to uniformly shuffle the top k template hits before
                parsing max_template_hits of them. Can be used to
                approximate DeepMind's training-time template subsampling
                scheme much more performantly.
            treat_pdb_as_distillation:
                Whether to assume that .pdb files in the data_dir are from
                the self-distillation set (and should be subjected to
                special distillation set preprocessing steps).
            mode:
                "train", "val", or "predict"
        """
        super().__init__()
        logging.warning(
            "You are using a simple dataset, which has clean metadata, skips MSA and template and only "
            "handles single sequence features. Therefore, the following parameters will be ignored:\n"
            "chain_data_cache_path, alignment_dir, alignment_index, template_mmcif_dir, template_release_dates_cache_path, max_template_date, kalign_binary_path, max_template_hits, obsolete_pdbs_file_path, shuffle_top_k_prefiltered"
        )
        self.data_dir = data_dir

        # self.chain_data_cache = None
        # if chain_data_cache_path is not None:
        #     with open(chain_data_cache_path, "r") as fp:
        #         self.chain_data_cache = json.load(fp)
        #     assert isinstance(self.chain_data_cache, dict)

        # self.alignment_dir = alignment_dir
        self.config = config
        self.treat_pdb_as_distillation = treat_pdb_as_distillation
        self.mode = mode
        # self.alignment_index = alignment_index
        self._output_raw = _output_raw
        self._structure_index = _structure_index

        self.supported_exts = [".cif", ".core", ".pdb"]

        valid_modes = ["train", "eval", "predict"]
        if mode not in valid_modes:
            raise ValueError(f"mode must be one of {valid_modes}")

        # if template_release_dates_cache_path is None:
        #     logging.warning(
        #         "Template release dates cache does not exist. Remember to run "
        #         "scripts/generate_mmcif_cache.py before running OpenFold"
        #     )

        # if alignment_index is not None:
        #     self._chain_ids = list(alignment_index.keys())
        # else:
        #     self._chain_ids = list(os.listdir(alignment_dir))
        self._chain_ids = [
            (d, c)
            for d in os.listdir(data_dir)
            if os.path.isdir(os.path.join(data_dir, d))
            for c in os.listdir(os.path.join(data_dir, d))
            if c.split(".")[-1] in self.supported_exts
        ]
        # self._chain_dirs, self._chain_ids = zip(*self._chain_paths)
        self.chain_data_cache = {
            c: {"cluster_size": 99, "seq": "A" * 99} for c in self._chain_ids
        }

        # if filter_path is not None:
        #     with open(filter_path, "r") as f:
        #         chains_to_include = set([l.strip() for l in f.readlines()])

        # self._chain_ids = [c for c in self._chain_ids if c in chains_to_include]

        self._chain_id_to_idx_dict = {
            chain: i for i, chain in enumerate(self._chain_ids)
        }

        # template_featurizer = templates.TemplateHitFeaturizer(
        #     mmcif_dir=template_mmcif_dir,
        #     max_template_date=max_template_date,
        #     max_hits=max_template_hits,
        #     kalign_binary_path=kalign_binary_path,
        #     release_dates_path=template_release_dates_cache_path,
        #     obsolete_pdbs_path=obsolete_pdbs_file_path,
        #     _shuffle_top_k_prefiltered=shuffle_top_k_prefiltered,
        # )

        # self.data_pipeline = DataPipeline(
        #     template_featurizer=template_featurizer,
        # )

        self.data_pipeline = DataPipeline(
            template_featurizer=None,
        )
        if not self._output_raw:
            self.feature_pipeline = feature_pipeline.FeaturePipeline(config)

    def _parse_mmcif(self, path, file_id, chain_id, alignment_dir, alignment_index):
        with open(path, "r") as f:
            mmcif_string = f.read()

        mmcif_object = mmcif_parsing.parse(file_id=file_id, mmcif_string=mmcif_string)

        # Crash if an error is encountered. Any parsing errors should have
        # been dealt with at the alignment stage.
        if mmcif_object.mmcif_object is None:
            raise list(mmcif_object.errors.values())[0]

        mmcif_object = mmcif_object.mmcif_object

        data = self.data_pipeline.process_mmcif(
            mmcif=mmcif_object,
            alignment_dir=alignment_dir,
            chain_id=chain_id,
            alignment_index=alignment_index,
        )

        return data

    def chain_id_to_idx(self, chain_id):
        return self._chain_id_to_idx_dict[chain_id]

    def idx_to_chain_id(self, idx):
        return self._chain_ids[idx]

    def __getitem__(self, idx):
        name = self.idx_to_chain_id(idx)

        # alignment_index = None
        # if self.alignment_index is not None:
        #     alignment_dir = self.alignment_dir
        #     alignment_index = self.alignment_index[name]

        if self.mode == "train" or self.mode == "eval":
            spl = name.rsplit("_", 1)
            if len(spl) == 2:
                file_id, chain_id = spl
            else:
                (file_id,) = spl
                chain_id = None

            path = os.path.join(self.data_dir, file_id)
            structure_index_entry = None
            if self._structure_index is not None:
                structure_index_entry = self._structure_index[name]
                assert len(structure_index_entry["files"]) == 1
                filename, _, _ = structure_index_entry["files"][0]
                ext = os.path.splitext(filename)[1]
            else:
                ext = None
                for e in self.supported_exts:
                    if os.path.exists(path + e):
                        ext = e
                        break

                if ext is None:
                    raise ValueError("Invalid file type")

            path += ext
            if ext == ".cif":
                data = self._parse_mmcif(
                    path,
                    file_id,
                    chain_id,
                    alignment_dir=None,
                    alignment_index=None,
                )
            elif ext == ".core":
                data = self.data_pipeline.process_core(
                    path,
                    alignment_dir=None,
                    alignment_index=None,
                )
            elif ext == ".pdb":
                structure_index = None
                if self._structure_index is not None:
                    structure_index = self._structure_index[name]
                data = self.data_pipeline.process_pdb(
                    pdb_path=path,
                    alignment_dir=None,
                    # is_distillation=self.treat_pdb_as_distillation,
                    is_distillation=True,
                    chain_id=chain_id,
                    alignment_index=None,
                    _structure_index=structure_index,
                )
            else:
                raise ValueError("Extension branch missing")
        else:
            path = os.path.join(name, name + ".fasta")
            data = self.data_pipeline.process_fasta(
                fasta_path=path,
                alignment_dir=None,
                alignment_index=None,
            )

        if self._output_raw:
            return data

        feats = self.feature_pipeline.process_features(data, self.mode)

        feats["batch_idx"] = torch.tensor(
            [idx for _ in range(feats["aatype"].shape[-1])],
            dtype=torch.int64,
            device=feats["aatype"].device,
        )

        return feats

    def __len__(self):
        return len(self._chain_ids)


class DataPipeline:
    """Assembles input features."""

    def __init__(
        self,
        template_featurizer: Optional[templates.TemplateHitFeaturizer],
    ):
        self.template_featurizer = template_featurizer

    def _parse_msa_data(
        self,
        alignment_dir: str,
        alignment_index: Optional[Any] = None,
    ) -> Mapping[str, Any]:
        raise NotImplementedError
        # msa_data = {}
        # if alignment_index is not None:
        #     fp = open(os.path.join(alignment_dir, alignment_index["db"]), "rb")

        #     def read_msa(start, size):
        #         fp.seek(start)
        #         msa = fp.read(size).decode("utf-8")
        #         return msa

        #     for (name, start, size) in alignment_index["files"]:
        #         ext = os.path.splitext(name)[-1]

        #         if ext == ".a3m":
        #             msa, deletion_matrix = parsers.parse_a3m(read_msa(start, size))
        #             data = {"msa": msa, "deletion_matrix": deletion_matrix}
        #         elif ext == ".sto":
        #             msa, deletion_matrix, _ = parsers.parse_stockholm(
        #                 read_msa(start, size)
        #             )
        #             data = {"msa": msa, "deletion_matrix": deletion_matrix}
        #         else:
        #             continue

        #         msa_data[name] = data

        #     fp.close()
        # else:
        #     for f in os.listdir(alignment_dir):
        #         path = os.path.join(alignment_dir, f)
        #         ext = os.path.splitext(f)[-1]

        #         if ext == ".a3m":
        #             with open(path, "r") as fp:
        #                 msa, deletion_matrix = parsers.parse_a3m(fp.read())
        #             data = {"msa": msa, "deletion_matrix": deletion_matrix}
        #         elif ext == ".sto":
        #             with open(path, "r") as fp:
        #                 msa, deletion_matrix, _ = parsers.parse_stockholm(fp.read())
        #             data = {"msa": msa, "deletion_matrix": deletion_matrix}
        #         else:
        #             continue

        #         msa_data[f] = data

        # return msa_data

    def _parse_template_hits(
        self, alignment_dir: str, alignment_index: Optional[Any] = None
    ) -> Mapping[str, Any]:
        all_hits = {}
        if alignment_index is not None:
            # print("============", alignment_dir, alignment_index, "==========")
            fp = open(os.path.join(alignment_dir, alignment_index["db"]), "rb")

            def read_template(start, size):
                fp.seek(start)
                return fp.read(size).decode("utf-8")

            for (name, start, size) in alignment_index["files"]:
                ext = os.path.splitext(name)[-1]

                if ext == ".hhr":
                    hits = parsers.parse_hhr(read_template(start, size))
                    all_hits[name] = hits

            fp.close()
        else:
            for f in os.listdir(alignment_dir):
                path = os.path.join(alignment_dir, f)
                ext = os.path.splitext(f)[-1]

                if ext == ".hhr":
                    with open(path, "r") as fp:
                        hits = parsers.parse_hhr(fp.read())
                    all_hits[f] = hits

        return all_hits

    def _get_msas(
        self,
        alignment_dir: str,
        input_sequence: Optional[str] = None,
        alignment_index: Optional[str] = None,
    ):
        raise NotImplementedError
        # msa_data = self._parse_msa_data(alignment_dir, alignment_index)
        # if len(msa_data) == 0:
        #     if input_sequence is None:
        #         raise ValueError(
        #             """
        #             If the alignment dir contains no MSAs, an input sequence
        #             must be provided.
        #             """
        #         )
        #     msa_data["dummy"] = {
        #         "msa": [input_sequence],
        #         "deletion_matrix": [[0 for _ in input_sequence]],
        #     }

        # msas, deletion_matrices = zip(
        #     *[(v["msa"], v["deletion_matrix"]) for v in msa_data.values()]
        # )

        # return msas, deletion_matrices

    def _process_msa_feats(
        self,
        alignment_dir: str,
        input_sequence: Optional[str] = None,
        alignment_index: Optional[str] = None,
    ) -> Mapping[str, Any]:
        raise NotImplementedError
        # msas, deletion_matrices = self._get_msas(
        #     alignment_dir, input_sequence, alignment_index
        # )
        # msa_features = make_msa_features(
        #     msas=msas,
        #     deletion_matrices=deletion_matrices,
        # )

        # return msa_features

    def process_fasta(
        self,
        fasta_path: str,
        alignment_dir: str,
        alignment_index: Optional[str] = None,
    ) -> FeatureDict:
        """Assembles features for a single sequence in a FASTA file"""
        with open(fasta_path) as f:
            fasta_str = f.read()
        input_seqs, input_descs = parsers.parse_fasta(fasta_str)
        if len(input_seqs) != 1:
            raise ValueError(f"More than one input sequence found in {fasta_path}.")
        input_sequence = input_seqs[0]
        input_description = input_descs[0]
        num_res = len(input_sequence)

        template_features = empty_template_feats(len(input_sequence))

        sequence_features = make_sequence_features(
            sequence=input_sequence,
            description=input_description,
            num_res=num_res,
        )

        # msa_features = self._process_msa_feats(
        #     alignment_dir, input_sequence, alignment_index
        # )
        msa_features = empty_msa_feats(input_sequence)

        return {**sequence_features, **msa_features, **template_features}

    def process_mmcif(
        self,
        mmcif: mmcif_parsing.MmcifObject,  # parsing is expensive, so no path
        alignment_dir: str,
        chain_id: Optional[str] = None,
        alignment_index: Optional[str] = None,
    ) -> FeatureDict:
        """
        Assembles features for a specific chain in an mmCIF object.

        If chain_id is None, it is assumed that there is only one chain
        in the object. Otherwise, a ValueError is thrown.
        """
        if chain_id is None:
            chains = mmcif.structure.get_chains()
            chain = next(chains, None)
            if chain is None:
                raise ValueError("No chains in mmCIF file")
            chain_id = chain.id

        mmcif_feats = make_mmcif_features(mmcif, chain_id)

        input_sequence = mmcif.chain_to_seqres[chain_id]
        # hits = self._parse_template_hits(alignment_dir, alignment_index)
        template_features = empty_template_feats(len(input_sequence))

        # msa_features = self._process_msa_feats(
        #     alignment_dir, input_sequence, alignment_index,
        # )
        msa_features = empty_msa_feats(input_sequence)

        return {**mmcif_feats, **template_features, **msa_features}

    def process_pdb(
        self,
        pdb_path: str,
        # alignment_dir: str,
        is_distillation: bool = True,
        chain_id: Optional[str] = None,
        _structure_index: Optional[str] = None,
        # alignment_index: Optional[str] = None,
    ) -> FeatureDict:
        """
        Assembles features for a protein in a PDB file.
        """
        if _structure_index is not None:
            db_dir = os.path.dirname(pdb_path)
            db = _structure_index["db"]
            db_path = os.path.join(db_dir, db)
            fp = open(db_path, "rb")
            _, offset, length = _structure_index["files"][0]
            fp.seek(offset)
            pdb_str = fp.read(length).decode("utf-8")
            fp.close()
        else:
            with open(pdb_path, "r") as f:
                pdb_str = f.read()

        protein_object = protein.from_pdb_string(pdb_str, chain_id)
        input_sequence = _aatype_to_str_sequence(protein_object.aatype)
        description = os.path.splitext(os.path.basename(pdb_path))[0].upper()
        pdb_feats = make_pdb_features(
            protein_object, description, is_distillation=is_distillation
        )

        # hits = self._parse_template_hits(alignment_dir, alignment_index)
        template_features = empty_template_feats(len(input_sequence))

        # msa_features = self._process_msa_feats(
        #     alignment_dir, input_sequence, alignment_index
        # )
        msa_features = empty_msa_feats(pdb_feats["aatype"])

        return {**pdb_feats, **template_features, **msa_features}

    def process_core(
        self,
        core_path: str,
        alignment_dir: str,
        alignment_index: Optional[str] = None,
    ) -> FeatureDict:
        """
        Assembles features for a protein in a ProteinNet .core file.
        """
        with open(core_path, "r") as f:
            core_str = f.read()

        protein_object = protein.from_proteinnet_string(core_str)
        input_sequence = _aatype_to_str_sequence(protein_object.aatype)
        description = os.path.splitext(os.path.basename(core_path))[0].upper()
        core_feats = make_protein_features(protein_object, description)

        # hits = self._parse_template_hits(alignment_dir, alignment_index)
        # template_features = make_template_features(
        #     input_sequence,
        #     hits,
        #     self.template_featurizer,
        # )
        template_features = empty_template_feats(len(input_sequence))

        # msa_features = self._process_msa_feats(alignment_dir, input_sequence)
        msa_features = empty_msa_feats(input_sequence)

        return {**core_feats, **template_features, **msa_features}

    def process_multiseq_fasta(
        self,
        fasta_path: str,
        super_alignment_dir: str,
        ri_gap: int = 200,
    ) -> FeatureDict:
        """
        Assembles features for a multi-sequence FASTA. Uses Minkyung Baek's
        hack from Twitter (a.k.a. AlphaFold-Gap).
        """
        with open(fasta_path, "r") as f:
            fasta_str = f.read()

        input_seqs, input_descs = parsers.parse_fasta(fasta_str)

        # No whitespace allowed
        input_descs = [i.split()[0] for i in input_descs]

        # Stitch all of the sequences together
        input_sequence = "".join(input_seqs)
        input_description = "-".join(input_descs)
        num_res = len(input_sequence)

        sequence_features = make_sequence_features(
            sequence=input_sequence,
            description=input_description,
            num_res=num_res,
        )

        seq_lens = [len(s) for s in input_seqs]
        total_offset = 0
        for sl in seq_lens:
            total_offset += sl
            sequence_features["residue_index"][total_offset:] += ri_gap

        # msa_list = []
        # deletion_mat_list = []
        # for seq, desc in zip(input_seqs, input_descs):
        #     alignment_dir = os.path.join(super_alignment_dir, desc)
        #     msas, deletion_mats = self._get_msas(alignment_dir, seq, None)
        #     msa_list.append(msas)
        #     deletion_mat_list.append(deletion_mats)

        # final_msa = []
        # final_deletion_mat = []
        # msa_it = enumerate(zip(msa_list, deletion_mat_list))
        # for i, (msas, deletion_mats) in msa_it:
        #     prec, post = sum(seq_lens[:i]), sum(seq_lens[i + 1 :])
        #     msas = [[prec * "-" + seq + post * "-" for seq in msa] for msa in msas]
        #     deletion_mats = [
        #         [prec * [0] + dml + post * [0] for dml in deletion_mat]
        #         for deletion_mat in deletion_mats
        #     ]

        #     assert len(msas[0][-1]) == len(input_sequence)

        #     final_msa.extend(msas)
        #     final_deletion_mat.extend(deletion_mats)

        # TODO: add no msa and no template options
        # msa_features = make_msa_features(
        #     msas=final_msa,
        #     deletion_matrices=final_deletion_mat,
        # )
        msa_features = empty_msa_feats(input_sequence)

        template_feature_list = []
        for seq, desc in zip(input_seqs, input_descs):
            alignment_dir = os.path.join(super_alignment_dir, desc)
            # hits = self._parse_template_hits(alignment_dir, alignment_index=None)
            # template_features = make_template_features(
            #     seq,
            #     hits,
            #     self.template_featurizer,
            # )
            template_features = empty_template_feats(len(seq))
            template_feature_list.append(template_features)

        template_features = unify_template_features(template_feature_list)

        return {
            **sequence_features,
            **msa_features,
            **template_features,
        }


class PDBSimpleSingleDataset(torch.utils.data.Dataset):
    """
    Structure data in mmCIF format (thus no structure index).
    No template and MSA.
    Has filtering and caching.
    """

    ext = ".cif"

    def __init__(
        self,
        chain_index_path: str,
        data_dir: str,
        config: mlc.ConfigDict,
        chain_data_cache_path: Optional[str] = None,
        filter_path: Optional[str] = None,
        mode: str = "train",
        _output_raw: bool = False,
    ):
        super().__init__()
        self.data_dir = data_dir
        # self.config = config
        self.mode = mode
        self.mode = mode
        self._output_raw = _output_raw

        with open(chain_index_path) as f:
            self._chain_ids = json.load(f)["data"]

        if filter_path is not None:
            with open(filter_path, "r") as f:
                chains_to_include = set([l.strip() for l in f.readlines()])

            self._chain_ids = [c for c in self._chain_ids if c in chains_to_include]

        if chain_data_cache_path is not None:
            with open(chain_data_cache_path, "r") as f:
                self.chain_data_cache = json.load(f)
            assert isinstance(self.chain_data_cache, dict)
        else:
            self.chain_data_cache = None

        if self.chain_data_cache is not None:
            # Filter to include only chains where we have structure data
            # (entries in chain_data_cache)
            # import pdb; pdb.set_trace()
            original_chain_ids = self._chain_ids
            self._chain_ids = [c for c in self._chain_ids if c in self.chain_data_cache]
            if len(self._chain_ids) < len(original_chain_ids):
                missing = [
                    c for c in original_chain_ids if c not in self.chain_data_cache
                ]
                max_to_print = 10
                missing_examples = ", ".join(missing[:max_to_print])
                if len(missing) > max_to_print:
                    missing_examples += ", ..."
                logging.warning(
                    "Removing %d alignment entries (%s) with no corresponding "
                    "entries in chain_data_cache (%s).",
                    len(missing),
                    missing_examples,
                    chain_data_cache_path,
                )

        self._chain_id_to_idx_dict = {
            chain: i for i, chain in enumerate(self._chain_ids)
        }

        self.data_pipeline = DataPipeline(template_featurizer=None)
        if not self._output_raw:
            self.feature_pipeline = feature_pipeline.FeaturePipeline(config)

    def chain_id_to_idx(self, chain_id):
        return self._chain_id_to_idx_dict[chain_id]

    def idx_to_chain_id(self, idx):
        return self._chain_ids[idx]

    def _parse_mmcif(self, path, file_id, chain_id):
        with open(path, "r") as f:
            mmcif_string = f.read()

        mmcif_object = mmcif_parsing.parse(file_id=file_id, mmcif_string=mmcif_string)

        # Crash if an error is encountered. Any parsing errors should have
        # been dealt with at the alignment stage.
        if mmcif_object.mmcif_object is None:
            raise list(mmcif_object.errors.values())[0]

        mmcif_object = mmcif_object.mmcif_object

        data = self.data_pipeline.process_mmcif(
            mmcif=mmcif_object,
            chain_id=chain_id,
            alignment_dir=None,
            alignment_index=None,
        )

        return data

    def __len__(self):
        return len(self._chain_ids)

    def __getitem__(self, idx):
        name = self.idx_to_chain_id(idx)

        # alignment_index = None
        # if self.alignment_index is not None:
        #     alignment_dir = self.alignment_dir
        #     alignment_index = self.alignment_index[name]

        if self.mode == "train" or self.mode == "eval":
            spl = name.rsplit("_", 1)
            if len(spl) == 2:
                file_id, chain_id = spl
            else:
                (file_id,) = spl
                chain_id = None

            path = os.path.join(self.data_dir, file_id)

            path += self.ext
            data = self._parse_mmcif(
                path,
                file_id,
                chain_id,
            )
        else:
            path = os.path.join(name, name + ".fasta")
            data = self.data_pipeline.process_fasta(
                fasta_path=path,
                alignment_dir=None,
                alignment_index=None,
            )

        if self._output_raw:
            return data

        feats = self.feature_pipeline.process_features(data, self.mode)

        feats["batch_idx"] = torch.tensor(
            [idx for _ in range(feats["aatype"].shape[-1])],
            dtype=torch.int64,
            device=feats["aatype"].device,
        )

        return feats


class OpenFoldSimpleSingleDataset(torch.utils.data.Dataset):
    """
    Structure data in PDB format. 270000 UniClust30 chains predicted with AF2.
    Directory structure as specified in https://docs.google.com/document/d/1R90-VJSLQEbot7tgXF3zb068Y1ZJAmsckQ_t2sJTv2c
    No template and MSA.
    Has caching but no filtering.
    """

    ext = ".pdb"

    def __init__(
        self,
        # chain_index_path: str,
        data_dir: str,
        config: mlc.ConfigDict,
        chain_data_cache_path: Optional[str] = None,
        # filter_path: Optional[str] = None,
        mode: str = "train",
        _output_raw: bool = False,
        _structure_index: Optional[Any] = None,
    ):
        super().__init__()
        self.data_dir = data_dir
        self.mode = mode
        self._output_raw = _output_raw

        self._chain_ids = [d for d in os.listdir(data_dir) if os.path.isdir(d)]

        if chain_data_cache_path is not None:
            with open(chain_data_cache_path, "r") as f:
                self.chain_data_cache = json.load(f)
            assert isinstance(self.chain_data_cache, dict)
        else:
            self.chain_data_cache = None
        self._structure_index = _structure_index

        # chain_data_cache is not used to filter data. There is no point of doing this for distillation dataset.

        # if self.chain_data_cache is not None:
        #     # Filter to include only chains where we have structure data
        #     # (entries in chain_data_cache)
        #     # import pdb; pdb.set_trace()
        #     original_chain_ids = self._chain_ids
        #     self._chain_ids = [c for c in self._chain_ids if c in self.chain_data_cache]
        #     if len(self._chain_ids) < len(original_chain_ids):
        #         missing = [
        #             c for c in original_chain_ids if c not in self.chain_data_cache
        #         ]
        #         max_to_print = 10
        #         missing_examples = ", ".join(missing[:max_to_print])
        #         if len(missing) > max_to_print:
        #             missing_examples += ", ..."
        #         logging.warning(
        #             "Removing %d alignment entries (%s) with no corresponding "
        #             "entries in chain_data_cache (%s).",
        #             len(missing),
        #             missing_examples,
        #             chain_data_cache_path,
        #         )

        self._chain_id_to_idx_dict = {
            chain: i for i, chain in enumerate(self._chain_ids)
        }

        self.data_pipeline = DataPipeline(template_featurizer=None)
        if not self._output_raw:
            self.feature_pipeline = feature_pipeline.FeaturePipeline(config)

    def chain_id_to_idx(self, chain_id):
        return self._chain_id_to_idx_dict[chain_id]

    def idx_to_chain_id(self, idx):
        return self._chain_ids[idx]

    def __len__(self):
        return len(self._chain_ids)

    def __getitem__(self, idx):
        name = self.idx_to_chain_id(idx)
        if self.mode == "train" or self.mode == "eval":
            spl = name.rsplit("_", 1)
            if len(spl) == 2:
                file_id, chain_id = spl
            else:
                (file_id,) = spl
                chain_id = None

            # ==== note this line ====
            path = os.path.join(self.data_dir, file_id, file_id)
            # ========================

            path += self.ext
            structure_index = None
            if self._structure_index is not None:
                structure_index = self._structure_index[name]
            data = self.data_pipeline.process_pdb(
                pdb_path=path,
                is_distillation=True,
                chain_id=chain_id,
                _structure_index=structure_index,
            )
        else:
            path = os.path.join(name, name + ".fasta")
            data = self.data_pipeline.process_fasta(
                fasta_path=path,
                alignment_dir=None,
                alignment_index=None,
            )

        if self._output_raw:
            return data

        feats = self.feature_pipeline.process_features(data, self.mode)

        feats["batch_idx"] = torch.tensor(
            [idx for _ in range(feats["aatype"].shape[-1])],
            dtype=torch.int64,
            device=feats["aatype"].device,
        )

        return feats


class AtlasSimpleSingleDataset(torch.utils.data.Dataset):
    """
    Structure data in PDB format. 270000 UniClust30 chains predicted with AF2.
    Directory structure as specified in https://docs.google.com/document/d/1R90-VJSLQEbot7tgXF3zb068Y1ZJAmsckQ_t2sJTv2c
    No template and MSA.
    Has caching but no filtering.
    """

    ext = ".pdb"

    def __init__(
        self,
        # chain_index_path: str,
        data_dir: str,
        config: mlc.ConfigDict,
        chain_data_cache_path: Optional[str] = None,
        # filter_path: Optional[str] = None,
        mode: str = "train",
        _output_raw: bool = False,
    ):
        super().__init__()
        self.data_dir = data_dir
        self.mode = mode
        self._output_raw = _output_raw

        self.collections = [d for d in os.listdir(data_dir) if os.path.isdir(d)]
        self._chain_ids = [f"{c}/{i}" for c in self.collections for i in os.listdir(c)]

        # if chain_data_cache_path is not None:
        #     with open(chain_data_cache_path, "r") as f:
        #         self.chain_data_cache = json.load(f)
        #     assert isinstance(self.chain_data_cache, dict)
        # else:
        #     self.chain_data_cache = None

        self._chain_id_to_idx_dict = {
            chain: i for i, chain in enumerate(self._chain_ids)
        }

        # self.data_pipeline = DataPipeline(template_featurizer=None)
        if not self._output_raw:
            self.feature_pipeline = feature_pipeline.FeaturePipeline(config)

    def chain_id_to_idx(self, chain_id):
        return self._chain_id_to_idx_dict[chain_id]

    def idx_to_chain_id(self, idx):
        return self._chain_ids[idx]

    def __len__(self):
        return len(self._chain_ids)

    def __getitem__(self, idx):
        name = self.idx_to_chain_id(idx)
        if self.mode == "train" or self.mode == "eval":
            # spl = name.rsplit("_", 1)
            # if len(spl) == 2:
            #     file_id, chain_id = spl
            # else:
            #     (file_id,) = spl
            #     chain_id = None

            # # ==== note this line ====
            # path = os.path.join(self.data_dir, file_id, file_id)
            # # ========================

            # path += self.ext
            # structure_index = None
            # if self._structure_index is not None:
            #     structure_index = self._structure_index[name]
            # data = self.data_pipeline.process_pdb(
            #     pdb_path=path,
            #     is_distillation=True,
            #     chain_id=chain_id,
            #     _structure_index=structure_index,
            # )
            data = self.make_decoy_compact_openfold(f"{self.data_dir}/{name}")
        else:
            raise NotImplementedError
            path = os.path.join(name, name + ".fasta")
            data = self.data_pipeline.process_fasta(
                fasta_path=path,
                alignment_dir=None,
                alignment_index=None,
            )

        if self._output_raw:
            return data

        feats = self.feature_pipeline.process_features(data, self.mode)

        feats["batch_idx"] = torch.tensor(
            [idx for _ in range(feats["aatype"].shape[-1])],
            dtype=torch.int64,
            device=feats["aatype"].device,
        )

        return feats

    @staticmethod
    def make_decoy_pdb_compact(
        pdb_path: str, return_internal: bool = False
    ) -> Dict[str, np.ndarray]:
        with open(pdb_path) as f:
            protein_object = protein.from_pdb_string(f.read())
        protein_id = os.path.splitext(os.path.basename(pdb_path))[0]
        aatype = protein_object.aatype
        sequence = _aatype_to_str_sequence(aatype)
        num_res = len(aatype)
        pdb_feats: dict = make_sequence_features(
            sequence=sequence, description=protein_id, num_res=num_res
        )
        pdb_feats["all_atom_positions"] = protein_object.atom_positions.astype(
            np.float32
        )
        # resolution, is_distillation are not needed.
        # confidence mask is not added on all_atom_mask.
        all_atom_mask = protein_object.atom_mask.astype(np.float32)
        pdb_feats["all_atom_mask"] = all_atom_mask
        pdb_feats.update(empty_template_feats(len(sequence)))
        pdb_feats.update(empty_msa_feats(sequence))

        dummy_feature_names = [
            "aatype",
            "residue_index",
            "msa",
            "num_alignments",
            "seq_length",
            "between_segment_residues",
            "deletion_matrix",
            "no_recycling_iters",
            # no template
            "all_atom_mask",
            "all_atom_positions",
            "resolution",
            "use_clamped_fape",
            "is_distillation",
        ]
        dummy_config = mlc.ConfigDict(
            {
                "common": {"use_templates": False},
                "train": {"crop_size": num_res, "supervised": True},
            }
        )
        # high_confidence = protein_object.b_factors > confidence_threshold
        # cfg, feature_names = make_data_config(dummy_config, mode="train", num_res=num_res)
        tensor_dict = np_to_tensor_dict(
            np_example=pdb_feats, features=dummy_feature_names
        )
        with torch.no_grad():
            # features = process_tensors_from_config()
            nonensembled = nonensembled_transform_fns(
                common_cfg=dummy_config["common"], mode_cfg=dummy_config["train"]
            )
            tensor_dict = compose(nonensembled)(tensor_dict)
            
        # now let's apply some assertions to make sure that decoy structure is indeed as simple as we thought.
        assert tensor_dict["seq_length"] == num_res, "sequence length not matched"
        assert (
            tensor_dict["residue_index"].numpy() == np.arange(num_res)
        ).all(), "residue index not trivial"
        assert (
            tensor_dict["between_segment_residues"] == 0
        ).all(), "between segment residues not trivial"

        torsion_sin, torsion_cos = (
            tensor_dict["torsion_angles_sin_cos"][:, 2:]
            .permute(2, 0, 1)
            # don't know why, but seems sometimes the value is out of this range and you get RuntimeWarning: invalid value encountered in arccos
            .clip(-1, 1)
            .numpy()
        )
        a_acos = np.arccos(torsion_cos)
        angle = np.degrees(a_acos)
        angle[torsion_sin < 0] = np.degrees(-a_acos)[torsion_sin < 0] % 360
        frame_rot_mat = tensor_dict["backbone_rigid_tensor"][:, :3, :3].numpy()
        frame_rot_euler = (
            R.from_matrix(frame_rot_mat)
            .as_euler("zyx", degrees=True)
            .astype(np.float32)
        )

        # check that accuracy is preserved, by circular consistency.
        # frame_rot_mat_back = R.from_euler("zyx", frame_rot_euler, degrees=True).as_matrix()
        # frame_rot_euler_back = R.from_matrix(frame_rot_mat_back).as_euler(
        #     "zyx", degrees=True
        # )
        # print(np.abs(frame_rot_euler - frame_rot_euler_back).max())

        frame_trans = tensor_dict["backbone_rigid_tensor"][:, :3, 3].numpy()

        confidence = (
            np.ma.masked_array(protein_object.b_factors, mask=1 - all_atom_mask)
            .max(axis=1, keepdims=True)
            .astype(np.float32)
        )
        # 1 + 5 + 3 + 3 = 12.
        res = {
            "aatype": tensor_dict["aatype"].numpy().astype(np.int8),
            "data": np.concatenate(
                [confidence, angle, frame_rot_euler, frame_trans], axis=1
            ),
        }
        return res, pdb_feats if return_internal else res

    @staticmethod
    def make_decoy_compact_openfold(
        compact_path: str, confidence_threshold: float = 0.5
    ) -> Dict[str, np.ndarray]:
        structure = np.load(compact_path)
        aatype = structure["aatype"]
        sequence = _aatype_to_str_sequence(aatype)
        data = structure["data"]
        confidence = data[:, 0]
        torsion_angle = data[:, 1:6]
        frame_rot_euler = data[:, 6:9]
        frame_trans = data[:, 9:12]

        torsion_angle = np.deg2rad(torsion_angle)
        # [num_res, 5, 2]
        torsion_sin_cos = np.concatenate(
            [
                np.full((aatype.shape[0], 2, 2), -999),  # dummy
                np.stack([np.sin(torsion_angle), np.cos(torsion_angle)], axis=2),
            ],
            axis=1,
        )
        rot_mats = R.from_euler("zyx", frame_rot_euler, degrees=True).as_matrix()
        backb_to_global = Rigid(
            Rotation(rot_mats=torch.from_numpy(rot_mats), quats=None),
            torch.from_numpy(frame_trans),
        )
        default_frames_ = torch.from_numpy(default_frames)
        group_idx_ = torch.from_numpy(group_idx)
        atom_mask_ = torch.from_numpy(atom_mask)
        lit_positions_ = torch.from_numpy(lit_positions)
        aatype_ = torch.from_numpy(aatype).long()

        pred_xyz = frames_and_literature_positions_to_atom14_pos(
            torsion_angles_to_frames(
                backb_to_global,
                torch.from_numpy(torsion_sin_cos),
                aatype_,
                default_frames_,
            ),
            aatype_,
            default_frames_,
            group_idx_,
            atom_mask_,
            lit_positions_,
        ).numpy()
        # [num_res, 37, 3]
        pred_xyz = atom14_to_atom37(pred_xyz, aatype)
        all_atom_mask = atom_mask_37[aatype]

        seq_len = len(aatype)
        res: dict = make_sequence_features(
            sequence=sequence,
            description=os.path.splitext(os.path.basename(compact_path))[0],
            num_res=seq_len,
        )
        res.update(
            {
                "all_atom_positions": pred_xyz,
                "all_atom_mask": (
                    all_atom_mask * (confidence > confidence_threshold)[:, None]
                ).astype(np.float32),
                "resolution": np.array([0.1]),
                "is_distillation": np.array([True]),
            }
        )
        res.update(empty_template_feats(seq_len))
        res.update(empty_msa_feats(sequence))
        return res


def make_sequence_features(
    sequence: str, description: str, num_res: int
) -> FeatureDict:
    """Construct a feature dict of sequence features."""
    features = {}
    features["aatype"] = rc.sequence_to_onehot(
        sequence=sequence,
        mapping=rc.restype_order_with_x,
        map_unknown_to_x=True,
    )
    features["between_segment_residues"] = np.zeros((num_res,), dtype=np.int32)
    features["domain_name"] = np.array([description.encode("utf-8")], dtype=np.object_)
    features["residue_index"] = np.array(range(num_res), dtype=np.int32)
    features["seq_length"] = np.array([num_res] * num_res, dtype=np.int32)
    features["sequence"] = np.array([sequence.encode("utf-8")], dtype=np.object_)
    return features


def make_mmcif_features(
    mmcif_object: mmcif_parsing.MmcifObject, chain_id: str
) -> FeatureDict:
    input_sequence = mmcif_object.chain_to_seqres[chain_id]
    description = "_".join([mmcif_object.file_id, chain_id])
    num_res = len(input_sequence)

    mmcif_feats = {}

    mmcif_feats.update(
        make_sequence_features(
            sequence=input_sequence,
            description=description,
            num_res=num_res,
        )
    )

    all_atom_positions, all_atom_mask = mmcif_parsing.get_atom_coords(
        mmcif_object=mmcif_object, chain_id=chain_id
    )
    mmcif_feats["all_atom_positions"] = all_atom_positions
    mmcif_feats["all_atom_mask"] = all_atom_mask

    mmcif_feats["resolution"] = np.array(
        [mmcif_object.header["resolution"]], dtype=np.float32
    )

    mmcif_feats["release_date"] = np.array(
        [mmcif_object.header["release_date"].encode("utf-8")], dtype=np.object_
    )

    mmcif_feats["is_distillation"] = np.array(0.0, dtype=np.float32)

    return mmcif_feats


def _aatype_to_str_sequence(aatype):
    return "".join([rc.restypes_with_x[aatype[i]] for i in range(len(aatype))])


def make_protein_features(
    protein_object: protein.Protein,
    description: str,
    _is_distillation: bool = False,
) -> FeatureDict:
    pdb_feats = {}
    aatype = protein_object.aatype
    sequence = _aatype_to_str_sequence(aatype)
    pdb_feats.update(
        make_sequence_features(
            sequence=sequence,
            description=description,
            num_res=len(protein_object.aatype),
        )
    )

    all_atom_positions = protein_object.atom_positions
    all_atom_mask = protein_object.atom_mask

    pdb_feats["all_atom_positions"] = all_atom_positions.astype(np.float32)
    pdb_feats["all_atom_mask"] = all_atom_mask.astype(np.float32)

    pdb_feats["resolution"] = np.array([0.0]).astype(np.float32)
    pdb_feats["is_distillation"] = np.array(1.0 if _is_distillation else 0.0).astype(
        np.float32
    )

    return pdb_feats


def make_pdb_features(
    protein_object: protein.Protein,
    description: str,
    is_distillation: bool = True,
    confidence_threshold: float = 50.0,
) -> FeatureDict:
    pdb_feats = make_protein_features(
        protein_object, description, _is_distillation=True
    )

    if is_distillation:
        high_confidence = protein_object.b_factors > confidence_threshold
        high_confidence = np.any(high_confidence, axis=-1)
        pdb_feats["all_atom_mask"] *= high_confidence[..., None]

    return pdb_feats


def make_atom14_masks(aatype: np.ndarray) -> Dict[str, np.ndarray]:
    """Construct denser atom positions (14 dimensions instead of 37)."""
    restype_atom14_to_atom37 = []
    restype_atom37_to_atom14 = []
    restype_atom14_mask = []

    for rt in rc.restypes:
        atom_names = rc.restype_name_to_atom14_names[rc.restype_1to3[rt]]
        restype_atom14_to_atom37.append(
            [(rc.atom_order[name] if name else 0) for name in atom_names]
        )
        atom_name_to_idx14 = {name: i for i, name in enumerate(atom_names)}
        restype_atom37_to_atom14.append(
            [
                (atom_name_to_idx14[name] if name in atom_name_to_idx14 else 0)
                for name in rc.atom_types
            ]
        )

        restype_atom14_mask.append([(1.0 if name else 0.0) for name in atom_names])

    # Add dummy mapping for restype 'UNK'
    restype_atom14_to_atom37.append([0] * 14)
    restype_atom37_to_atom14.append([0] * 37)
    restype_atom14_mask.append([0.0] * 14)

    # restype_atom14_to_atom37 = torch.tensor(
    #     restype_atom14_to_atom37,
    #     dtype=torch.int32,
    #     device=aatype.device,
    # )
    # restype_atom37_to_atom14 = torch.tensor(
    #     restype_atom37_to_atom14,
    #     dtype=torch.int32,
    #     device=aatype.device,
    # )
    # restype_atom14_mask = torch.tensor(
    #     restype_atom14_mask,
    #     dtype=torch.float32,
    #     device=aatype.device,
    # )
    # protein_aatype = aatype.to(torch.long)
    restype_atom14_to_atom37 = np.array(restype_atom14_to_atom37)
    restype_atom37_to_atom14 = np.array(restype_atom37_to_atom14)
    restype_atom14_mask = np.array(restype_atom14_mask)

    # create the mapping for (residx, atom14) --> atom37, i.e. an array
    # with shape (num_res, 14) containing the atom37 indices for this protein
    residx_atom14_to_atom37 = restype_atom14_to_atom37[aatype]
    residx_atom14_mask = restype_atom14_mask[aatype]

    res = {}
    res["atom14_atom_exists"] = residx_atom14_mask
    res["residx_atom14_to_atom37"] = residx_atom14_to_atom37

    # create the gather indices for mapping back
    residx_atom37_to_atom14 = restype_atom37_to_atom14[aatype]
    res["residx_atom37_to_atom14"] = residx_atom37_to_atom14

    # create the corresponding mask
    # restype_atom37_mask = torch.zeros(
    #     [21, 37], dtype=torch.float32, device=aatype.device
    # )
    restype_atom37_mask = np.zeros([21, 37], dtype=np.float32)
    for restype, restype_letter in enumerate(rc.restypes):
        restype_name = rc.restype_1to3[restype_letter]
        atom_names = rc.residue_atoms[restype_name]
        for atom_name in atom_names:
            atom_type = rc.atom_order[atom_name]
            restype_atom37_mask[restype, atom_type] = 1

    residx_atom37_mask = restype_atom37_mask[aatype]
    res["atom37_atom_exists"] = residx_atom37_mask

    return res


def atom14_to_atom37(atom14: np.ndarray, aatype: np.ndarray) -> np.ndarray:
    # atom16: [num_res, 14, 3]
    batch: Dict[str, np.ndarray] = make_atom14_masks(aatype)
    atom37_data = batched_gather(
        torch.from_numpy(atom14),
        torch.from_numpy(batch["residx_atom37_to_atom14"]),
        dim=-2,
        no_batch_dims=len(atom14.shape[:-2]),
    )

    atom37_data = atom37_data * batch["atom37_atom_exists"][..., None]

    return atom37_data.numpy()


# def make_template_features(
#     input_sequence: str,
#     hits: Sequence[Any],
#     template_featurizer: Any,
#     query_pdb_code: Optional[str] = None,
#     query_release_date: Optional[str] = None,
# ) -> FeatureDict:
#     hits_cat = sum(hits.values(), [])
#     if len(hits_cat) == 0 or template_featurizer is None:
#         template_features = empty_template_feats(len(input_sequence))
#     else:
#         templates_result = template_featurizer.get_templates(
#             query_sequence=input_sequence,
#             query_pdb_code=query_pdb_code,
#             query_release_date=query_release_date,
#             hits=hits_cat,
#         )
#         template_features = templates_result.features

#         # The template featurizer doesn't format empty template features
#         # properly. This is a quick fix.
#         if template_features["template_aatype"].shape[0] == 0:
#             template_features = empty_template_feats(len(input_sequence))

#     return template_features


def unify_template_features(
    template_feature_list: Sequence[FeatureDict],
) -> FeatureDict:
    out_dicts = []
    seq_lens = [fd["template_aatype"].shape[1] for fd in template_feature_list]
    for i, fd in enumerate(template_feature_list):
        out_dict = {}
        n_templates, n_res = fd["template_aatype"].shape[:2]
        for k, v in fd.items():
            seq_keys = [
                "template_aatype",
                "template_all_atom_positions",
                "template_all_atom_mask",
            ]
            if k in seq_keys:
                new_shape = list(v.shape)
                assert new_shape[1] == n_res
                new_shape[1] = sum(seq_lens)
                new_array = np.zeros(new_shape, dtype=v.dtype)

                if k == "template_aatype":
                    new_array[..., rc.HHBLITS_AA_TO_ID["-"]] = 1

                offset = sum(seq_lens[:i])
                new_array[:, offset : offset + seq_lens[i]] = v
                out_dict[k] = new_array
            else:
                out_dict[k] = v

        chain_indices = np.array(n_templates * [i])
        out_dict["template_chain_index"] = chain_indices

        if n_templates != 0:
            out_dicts.append(out_dict)

    if len(out_dicts) > 0:
        out_dict = {
            k: np.concatenate([od[k] for od in out_dicts]) for k in out_dicts[0]
        }
    else:
        out_dict = empty_template_feats(sum(seq_lens))

    return out_dict


def empty_template_feats(n_res) -> FeatureDict:
    return {
        "template_aatype": np.zeros((0, n_res)).astype(np.int64),
        "template_all_atom_positions": np.zeros((0, n_res, 37, 3)).astype(np.float32),
        "template_sum_probs": np.zeros((0, 1)).astype(np.float32),
        "template_all_atom_mask": np.zeros((0, n_res, 37)).astype(np.float32),
    }


def empty_msa_feats(input_sequence, num_msas_all=None) -> FeatureDict:
    num_res = len(input_sequence)

    single_sequence_msa = data_pipeline.make_msa_features(
        [[input_sequence]], [[[0 for _ in input_sequence]]]
    )

    if num_msas_all is None:
        return single_sequence_msa
    else:
        return {
            "msa": np.concatenate(
                [
                    single_sequence_msa["msa"],
                    np.full((num_msas_all - 1, num_res), -999, dtype=np.int32),
                ],
                axis=0,
            ),
            "deletion_matrix_int": np.zeros(
                (num_msas_all, num_res), -999, dtype=np.int32
            ),
            "num_alignment": np.full(num_msas_all, -999, dtype=np.int32),
        }
